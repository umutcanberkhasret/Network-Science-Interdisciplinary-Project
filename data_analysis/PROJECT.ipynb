{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4bkwQoHr9i3"
   },
   "source": [
    "# Laboratory 1: setting up Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21NaYdhRr9i7"
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNFOaZPIxyS2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKEN'] = 'your token here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1905,
     "status": "ok",
     "timestamp": 1642332005355,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "GjUQcJpfrecr",
    "outputId": "9499c17f-4bae-4c4a-99dc-acd97391dba7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nwy6MeGur9i-"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ge5EaEur9jA"
   },
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd \n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrXv204vyooY"
   },
   "source": [
    "If you need to download a library, use the following code, just specify the name of the library you need (here we downloaded emoji library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5214,
     "status": "ok",
     "timestamp": 1642327240593,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "N6RYD_oCoJ3C",
    "outputId": "deb44834-ba26-482a-bfcf-dc0b22f7c14d"
   },
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYIP9Ta4r9jR"
   },
   "source": [
    "### Set up headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKWoILn7AMzj"
   },
   "outputs": [],
   "source": [
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_buwaoAyPLM"
   },
   "outputs": [],
   "source": [
    "headers = create_headers(os.environ['TOKEN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8Pb9MXWr9jd"
   },
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUk4LQkEzbqg"
   },
   "source": [
    "Date format and other parameter explanations available here:\n",
    "\n",
    "https://developer.twitter.com/en/docs/twitter-api/premium/search-api/api-reference/premium-search#DataParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90Ukr82M_N39"
   },
   "outputs": [],
   "source": [
    "def create_url(keyword, start_date, end_date, env_label, endpoint=\"fullarchive\"):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/1.1/tweets/search/{}.json\".format(endpoint+\"/\"+env_label) \n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword, 'fromDate': start_date, 'toDate': end_date}\n",
    "    return (search_url, query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJD9pEHwAk84"
   },
   "outputs": [],
   "source": [
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    if next_token is not None and next_token != '':\n",
    "      params['next'] = next_token\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Jgw8uZd0K5u"
   },
   "outputs": [],
   "source": [
    "def get_data(keyword, start_time, end_time, next_token, env_label, endpoint):\n",
    "  results = []\n",
    "  dCounter = 0\n",
    "  while next_token is not None and dCounter < 10:  #change dCounter to 10 to retrieve 1k data\n",
    "    ##this part here for one request\n",
    "    url = create_url(keyword, start_time,end_time, env_label, endpoint)\n",
    "    json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "    \n",
    "    if \"results\" in json_response:\n",
    "      results.extend(json_response[\"results\"])\n",
    "    ### up until this point\n",
    "    if \"next\" in json_response:\n",
    "        next_token = json_response[\"next\"]\n",
    "        dCounter += 1\n",
    "    else:\n",
    "      next_token = None\n",
    "    time.sleep(1)\n",
    "  \n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyXiwYZCl8H3"
   },
   "outputs": [],
   "source": [
    "def get_single_response(keyword, start_time, end_time, env_label, endpoint):\n",
    "  #endpoint can be fullarchive or 30day\n",
    "  results = []\n",
    "  url = create_url(keyword, start_time,end_time, env_label, endpoint)\n",
    "  json_response = connect_to_endpoint(url[0], headers, url[1])\n",
    "  \n",
    "  if \"results\" in json_response:\n",
    "    results.extend(json_response[\"results\"])\n",
    "  \n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSt4xdwfmHcd"
   },
   "outputs": [],
   "source": [
    "#if you are using the 30day endpoint, make sure you specify dates that are within 30day range!\n",
    "#To get small data of size \"100\" use, get_single_response\n",
    "#change the hashtags to your liking\n",
    "# with \"get_data\" we are querying for 1k data - see the definition of get_data func above\n",
    "tweets = get_data(\"(gulpanra OR JusticeforGulPanra OR StopGenocideOfTransgenders OR TransLivesMatter) lang:en\", \"202001010000\", \"202012310000\", \"\",\"NSdev\", \"fullarchive\")\n",
    "#tweets = get_single_response(\"(EndTransViolence OR BeelaCrisis OR JusticeForBijlee OR JusticeForToffi OR JusticeforGulPanra OR TransLivesMatter) lang:en\", \"202001010000\", \"202112310000\",\"NSdev\", \"fullarchive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zA-AQNsp08OX"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tQoczST6dQ8"
   },
   "source": [
    "### Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1642330931360,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "qcWGG5Rz39Gr",
    "outputId": "64389f97-d0f6-4524-c131-a1f79f585040"
   },
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1642330947666,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "Qy1AZe4O4T7C",
    "outputId": "77305af0-5353-4291-db09-8b714ae3f6be",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUGfn_G3qcLV"
   },
   "source": [
    "# Laboratory 2: working with Twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6mSYSqKqh7f"
   },
   "source": [
    "First, we want to convert the data into Pandas DataFrame. This format enables us easy manipulation of the data as well as saving/loading data.\n",
    "\n",
    "Since we have our tweets saved as a list of dictionaries, we can easily convert it to DataFrame by executing the cell blow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1C940uZnWFA"
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "executionInfo": {
     "elapsed": 605,
     "status": "ok",
     "timestamp": 1642330958035,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "3QKr-0nRiXHa",
    "outputId": "5750c89d-9914-4afc-934d-9df3ab7c5dea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVXP-fDrtMae"
   },
   "source": [
    "### Saving the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kmi5SprXrEOM"
   },
   "source": [
    "Once we have our Tweets as a DataFrame it is a good idea to save it on the disk. \n",
    "\n",
    "Be mindful of the fact that the storage of a Colab notebook is deleted everytime runtime is interrupted or restarted, so you need to manually download it to your computer or mount your Google Drive and save it there (this option is unavailable if you're using university's email account for Drive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1642332022585,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "PqdepSBprmvl"
   },
   "outputs": [],
   "source": [
    "path = \"Desktop\\network proje\" #enter the path to your Drive or leave this as default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmtD5-THsUJp"
   },
   "source": [
    "We can save it as a comma-separated values file, which enables opening it in a spreadsheet editor and inspecting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 780,
     "status": "ok",
     "timestamp": 1642334657198,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "ex1E8v-qrkeh"
   },
   "outputs": [],
   "source": [
    "tweets_df.to_csv(path+\"trans_case1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFcy4tKls7qD"
   },
   "source": [
    "In order to preserve datatypes, we should save it as a parquet or pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1642332039462,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "hUqdVEGksm7n"
   },
   "outputs": [],
   "source": [
    "tweets_df.to_pickle(path+\"trans_case1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H02ZoRcGtpaY"
   },
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWhc-_nNtsnT"
   },
   "source": [
    "If you want to load the results you have previously saved, simply execute the next code, specifying the path to the file.\n",
    "\n",
    "You will need to either upload it to the Colab workspace or copy the path to the file on Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": {
     "elapsed": 266,
     "status": "error",
     "timestamp": 1642331030407,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "1LlenT11t_Xp",
    "outputId": "82831245-77b3-4da1-c21f-d2c30751b1dc"
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.read_pickle(\"trans_case1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 974
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1641475754729,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "RNgY35TclZC6",
    "outputId": "a95cb18e-ddc6-4c95-d618-529616b06ad2"
   },
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9ItAgz7uga9"
   },
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSvM1_xzu9Uc"
   },
   "source": [
    "In our dataframe we have the entire Tweet object. Some columns that might be of particular interest to us are: \n",
    "\n",
    "*   created_at - date when Tweet was posted\n",
    "*   id/id_str - unique Tweet identifiers\n",
    "*   text - the content of the Tweet\n",
    "*   user - information about the user who posted the Tweet\n",
    "*   retweeted_status  - information about the original Tweet\n",
    "*   quote/reply/retweet/favorite count - Tweet metrics\n",
    "*   entities - hashtags, urls, user_mentions present in Tweet\n",
    "\n",
    "We can filter the dataframe and keep only columns we are interested in. You can pick which columns you'd like to keep and put them int the column_list below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZYiG40aumbk"
   },
   "outputs": [],
   "source": [
    "#Everything is filtered below in Umut's code, no need to do it now\n",
    "\n",
    "#tweets_filtered = tweets_df.copy() #it's a good idea to work on the copy of original dataframe, so we can always go back to it if we mess something up\n",
    "#column_list = [\"created_at\", \"id_str\", \"text\", \"user\", \"retweeted_status\", \"quote_count\", \"reply_count\", \"retweet_count\", \"favorite_count\", \"entities\"]\n",
    "#tweets_filtered = tweets_filtered[column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 956
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1641475763262,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "qQ90m-FpxM9N",
    "outputId": "6da9745e-64fc-4e40-d17b-b274056da8ef",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessData(df):\n",
    "    for index, row in df.iterrows():\n",
    "        # get full_text of the tweet\n",
    "        if row['truncated']:\n",
    "            tweet = {\"full_text\": ast.literal_eval(row['extended_tweet'])['full_text']}\n",
    "        else:\n",
    "            # if the tweet is retweeted, this status will not be na\n",
    "            if ('retweeted_status' in row) and (not pd.isna(row['retweeted_status'])):\n",
    "                originalTweet = ast.literal_eval(row['retweeted_status'])\n",
    "                userTagName = originalTweet['user']['screen_name']\n",
    "\n",
    "                # if the text is truncated\n",
    "                if originalTweet['truncated']:\n",
    "                    tweet = {\"full_text\": \"RT @\" + userTagName + \": \" + originalTweet['extended_tweet']['full_text']}\n",
    "                else:\n",
    "                    tweet = {\"full_text\": \"RT @\" + userTagName + \": \" + originalTweet['text']}\n",
    "\n",
    "                del originalTweet\n",
    "                del userTagName\n",
    "\n",
    "            # if the tweet is original and not truncated\n",
    "            else:\n",
    "                tweet = {\"full_text\": row['text']}\n",
    "\n",
    "        # get Username & User Tag & Location & Verification status of account\n",
    "        tweet[\"username\"] = ast.literal_eval(row['user'])['name']\n",
    "        tweet[\"screen_name\"] = ast.literal_eval(row['user'])['screen_name']\n",
    "        tweet[\"location\"] = ast.literal_eval(row['user'])['location']\n",
    "        tweet[\"is_verified\"] = ast.literal_eval(row['user'])['verified']\n",
    "\n",
    "        # get Quote & Fav & RT & Reply counts\n",
    "        tweet[\"quote_count\"] = row['quote_count']\n",
    "        tweet[\"favorite_count\"] = row['quote_count']\n",
    "        tweet[\"retweet_count\"] = row['retweet_count']\n",
    "        tweet[\"reply_count\"] = row['reply_count']\n",
    "\n",
    "        # get Tweet creation date&time\n",
    "        tweet[\"created_at\"] = row['created_at']\n",
    "\n",
    "        # get TweetID\n",
    "        tweet[\"tweet_id\"] = row['id']\n",
    "\n",
    "        tweets.append(tweet)\n",
    "\n",
    "        del tweet\n",
    "        del row\n",
    "        del index\n",
    "\n",
    "\n",
    "def countNumberOfRetweetedTweets(tweets):\n",
    "    # Separate RTs and original tweets\n",
    "    for tweet in tweets:\n",
    "        if bool(re.match(r'(RT @[\\w]+:)', tweet[\"full_text\"])):\n",
    "            rts.append(tweet)\n",
    "        else:\n",
    "            non_rt.append(tweet)\n",
    "        del tweet\n",
    "\n",
    "\n",
    "def countTweetsFromVerifiedAccounts(tweets):\n",
    "    # Distinguish tweets according to their account verification status    \n",
    "    for tweet in tweets:\n",
    "        if tweet[\"is_verified\"]:\n",
    "            tweets_from_verified_accounts.append(tweet)\n",
    "        else:\n",
    "            tweets_from_regular_accounts.append(tweet)\n",
    "\n",
    "        del tweet\n",
    "\n",
    "\n",
    "# filter locations according to city names given in listOfCities\n",
    "def filterLocations(tweets, listOfCities):\n",
    "    filtered_tweets = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        for index, city in listOfCities.iterrows():\n",
    "            if city['city'] in tweet['location']:\n",
    "                filtered_tweets.append(tweet)\n",
    "                break\n",
    "    return filtered_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('trans_case1.csv')\n",
    "df2 = pd.read_csv('trans_case_mix.csv')\n",
    "df3 = pd.read_csv('trans_case3.csv')\n",
    "tweets = []\n",
    "tweets_from_verified_accounts = []\n",
    "tweets_from_regular_accounts = []\n",
    "\n",
    "rts = []\n",
    "non_rt = []\n",
    "\n",
    "#pakistanCities = pd.read_csv('pakistanCities.csv')\n",
    "#egyptCities = pd.read_csv('egyptCities.csv')\n",
    "#dfs = [df1, df2 ,df3]\n",
    "\n",
    "# all data of 3 different cases are concataneted \n",
    "dfs = pd.concat(map(pd.read_csv, ['trans_case1.csv', 'trans_case_mix.csv','trans_case3.csv']))\n",
    "\n",
    "#index column was not incrementing from 0 through 3000 for 3k data. So we resetted it to have an index column of 0-2999\n",
    "dfs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_final = dfs.copy() # to work on a copy, copied the original df\n",
    "preProcessData(df_final) # full text, column filtering are applied to the df\n",
    "countNumberOfRetweetedTweets(tweets) # count of Retweeted Tweets\n",
    "countTweetsFromVerifiedAccounts(tweets) # count of Tweets From Verified Accounts\n",
    "\n",
    "\"\"\"\n",
    "# preprocess all the given hashtags and build \n",
    "for df_instance in dfs:\n",
    "    preProcessData(df_instance)\n",
    "\n",
    "    del df_instance\n",
    "\n",
    "countNumberOfRetweetedTweets(tweets)\n",
    "countTweetsFromVerifiedAccounts(tweets)\n",
    "\n",
    "#original_tweets_from_pakistan = filterLocations(non_rt, pakistanCities)\n",
    "#all_tweets_from_pakistan = filterLocations(tweets, pakistanCities)\n",
    "\"\"\"\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame of whole network and in the last line, it saved it as a csv file\n",
    "tweets_df = pd.DataFrame(tweets)\n",
    "tweets_df\n",
    "tweets_df.to_csv(\"trans_full_network.csv\", index=False) # change the name according to your case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_from_verified_accounts_df = pd.DataFrame(tweets_from_verified_accounts)\n",
    "tweets_from_verified_accounts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame of tweets from regular accounts\n",
    "tweets_from_regular_accounts_df = pd.DataFrame(tweets_from_regular_accounts)\n",
    "tweets_from_regular_accounts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retweeted tweets\n",
    "len(rts) # length of the rt tweets\n",
    "rts_df = pd.DataFrame(rts)\n",
    "rts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not retweeted tweets, basically the remaining tweets\n",
    "len(non_rt) # length of not rt tweets\n",
    "original_tweets_df = pd.DataFrame(non_rt)\n",
    "original_tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPgzBUJj0SZU"
   },
   "source": [
    "## Extracting words/hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CygaLHAS0nzP"
   },
   "source": [
    "There are many ways to build networks from the data we download from Twitter.\n",
    "\n",
    "One possibility is to have a bipartite network of Tweets and words/hashtags and then observe word, hashtag or word-hashtag projections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1nTCbRc0-__"
   },
   "source": [
    "### Extracting words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pGnokgK1jma"
   },
   "source": [
    "In order to extract words, we first need to clean the Tweet text. This way we will remove punctuation, hashtags/mentions/urls (they are preserved in the entity column anyway). We will also turn all letters to lowercase.\n",
    "\n",
    "You can also consider removing stopwords, removing words that are not in the english language corpora, lematizing the words, etc. I suggest you research nltk library and its possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5746Mq918dG"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8Nrv5jv1e5W"
   },
   "outputs": [],
   "source": [
    "def cleaner(tweet):\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) # remove mentions\n",
    "    tweet = re.sub(\"#[A-Za-z0-9]+\", \"\",tweet) # remove hashtags\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) # remove http links\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    tweet = str.lower(tweet) #to lowercase\n",
    "    table = str.maketrans(dict.fromkeys(string.punctuation)) \n",
    "    tweet = tweet.translate(table)# remove punctuation         \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1641475773925,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "fEhs-tyy2naZ",
    "outputId": "8d6ccacf-57db-4492-b35b-0d5e0de5f655"
   },
   "outputs": [],
   "source": [
    "tweets_filtered[\"clean_text\"] = tweets_df[\"full_text\"].map(cleaner)\n",
    "print(tweets_filtered['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1_lSQZ85rT_"
   },
   "source": [
    "We are going to loop through the dataframe and then through the words in the clean text. We are going to add the words as keys to dictionary and use their frequencies as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiBKlJhV3d_S"
   },
   "outputs": [],
   "source": [
    "#initialize an empty dict\n",
    "unique_words = {}\n",
    "for row in tweets_filtered.clean_text:\n",
    "  for word in row.split(\" \"):\n",
    "    #if the word is encountered for the first time add to dict as key and set its value to 0\n",
    "    unique_words.setdefault(word,0)\n",
    "    #increase the value (i.e the count) of the word by 1 every time it is encountered\n",
    "    unique_words[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1641475792443,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "8k7riE1z7F6r",
    "outputId": "cce7212b-5934-4ffc-a44f-564ba16bf119"
   },
   "outputs": [],
   "source": [
    "#remove empty word\n",
    "unique_words.pop(\"\")\n",
    "#remove word 'rt'\n",
    "unique_words.pop(\"rt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVYvDBQu57If"
   },
   "source": [
    "We can inspect the words as a dataframe. \n",
    "\n",
    "\n",
    "You can always save this dataframe as .csv for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1641475796119,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "_xI5aGQM53u4",
    "outputId": "c81c1993-af77-4cce-bb8f-4beda4a36128"
   },
   "outputs": [],
   "source": [
    "#In the last line, it saves word count df as csv file. It's better to save it so that maybe we'll use it later for the presentation\n",
    "uw_df = pd.DataFrame.from_dict(unique_words, orient='index').reset_index()\n",
    "print(uw_df)\n",
    "uw_df.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
    "uw_df.sort_values(by=['Count'], ascending=False, inplace=True)\n",
    "uw_df\n",
    "uw_df.to_csv(\"full_network_word_count.csv\", index=False) # change the name to your liking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4R-j-FN7Rgo"
   },
   "source": [
    "### Extracting the hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8SfJwmf9GU2"
   },
   "source": [
    "We are going to loop through the dataframe and then through the hashtags in the entities. We are going to add the hashtags as keys to dictionary and use their frequencies as values. At the same time, we are going to save them in a list and add them to a separate column to facilitate our future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1641475805775,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "TEMUaIdCBlc_",
    "outputId": "213935de-889d-4e60-de67-c34545919ab2"
   },
   "outputs": [],
   "source": [
    "# no need to run this cell. We already have screen_name column by this point\n",
    "\n",
    "\"\"\"\n",
    "#TASK-1\n",
    "\n",
    "sc_name = {}\n",
    "tweets_filtered[\"screen_name\"] = \"\"\n",
    "\n",
    "for idx, row in tweets_filtered.iterrows():\n",
    "  screen_name_list = []\n",
    "  for user_mentions in row[\"entities\"][\"user_mentions\"]:\n",
    "    sc_name.setdefault(user_mentions['screen_name'], 0)\n",
    "    sc_name[user_mentions[\"screen_name\"]] += 1\n",
    "    screen_name_list.append(user_mentions[\"screen_name\"])\n",
    "  tweets_filtered.at[idx,\"screen_names\"] = screen_name_list\n",
    "sc_name\n",
    "\n",
    "sc_df = pd.DataFrame.from_dict(sc_name, orient='index').reset_index()\n",
    "sc_df.rename(columns = {'index':'Username', 0:'Count'}, inplace=True)\n",
    "sc_df.sort_values(by=['Count'], ascending=False, inplace=True)\n",
    "sc_df\n",
    "#END OF TASK-1\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1641475855692,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "hHyc_WrGt5Tj",
    "outputId": "d39c7ea7-5dfb-4e63-9154-5f747eaf69ca"
   },
   "outputs": [],
   "source": [
    "unique_hashtags = {}\n",
    "#Adds hashtag column in the filtered data frame\n",
    "tweets_filtered[\"hashtags\"] = \"\"\n",
    "\n",
    "for idx, row in tweets_filtered.iterrows():\n",
    "  hashtag_list = []\n",
    "  for hashtag in row[\"entities\"][\"hashtags\"]:\n",
    "    unique_hashtags.setdefault(\"#\"+hashtag[\"text\"], 0)\n",
    "    unique_hashtags['#'+hashtag[\"text\"]] += 1\n",
    "    hashtag_list.append(hashtag[\"text\"])\n",
    "  tweets_filtered.at[idx,\"hashtags\"] = hashtag_list\n",
    "unique_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCb7arJqUXOu"
   },
   "outputs": [],
   "source": [
    "uh_df = pd.DataFrame.from_dict(unique_hashtags, orient='index').reset_index()\n",
    "uh_df.rename(columns = {'index':'Hashtag', 0:'Count'}, inplace=True)\n",
    "uh_df.sort_values(by=['Count'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1641475861140,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "31Ydx_RbWSGc",
    "outputId": "34bfc0b7-26a6-453e-ac9a-cc42a30ab758"
   },
   "outputs": [],
   "source": [
    "#In the last line, it saves hashtag count df as csv file. It's better to save it so that maybe we'll use it later for the presentation\n",
    "uh_df\n",
    "uh_df.to_csv(\"full_network_hashtag_count.csv\", index=False) # change the name to your liking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPaVj1tj9Uw6"
   },
   "source": [
    "## Building the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt2co2ep9YCd"
   },
   "source": [
    "We are going to use the networkx library, which is a Python library that enables network science analysis of the data.\n",
    "\n",
    "We are going to use it to create our network and extract edgelist from it, since we can easily import it to Gephi (a software we are going to see in visualization labs).\n",
    "\n",
    "However, it offers implemented algorithms for analysis (for example PageRank) that you can use out-of-box to analyze your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnd62ng6-GLW"
   },
   "source": [
    "But first, we will loop through our dataframe and connect words and hashtags if they appear together in the same Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BooMyc6-1JWa"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adLbCz86M7SR"
   },
   "outputs": [],
   "source": [
    "uh = unique_hashtags.keys()\n",
    "uw = unique_words.keys()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1641475877898,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "LZItEvJqKVE5",
    "outputId": "24b317f5-f173-46c3-9215-fa7aef439e39"
   },
   "outputs": [],
   "source": [
    "# No need to run this cell\n",
    "\n",
    "\"\"\"\n",
    "#SELEN\n",
    "sc=sc_name.keys()\n",
    "sc\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1641476573042,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "UHuQ3rRXOA5_",
    "outputId": "092a9bcf-4595-4e5d-bea0-b82794f69278"
   },
   "outputs": [],
   "source": [
    "#It creates pairs from all words.\n",
    "# From this cell on, we will create 3 different networks:\n",
    "# Network1 = words+hashtags\n",
    "#Network2 = words only\n",
    "#Network3 = hashtags only\n",
    "#This cell is for Network1\n",
    "network = {}\n",
    "network_key = 0\n",
    "for index, row in tweets_filtered.iterrows():\n",
    "    #hashtags extracted from Tweet do not have the # sign in front of them but we will add it to differentiate hashtags from words\n",
    "    combined_list = ['#'+hashtag for hashtag in row[\"hashtags\"] if '#'+hashtag in unique_hashtags] + [word for word in str.split(row[\"clean_text\"], \" \") if word in uw]\n",
    "    #itertool product creates Cartesian product of each element in the combined list\n",
    "    for pair in itertools.product(combined_list, combined_list):\n",
    "        #exclude self-loops and count each pair only once because our graph is undirected and we do not take self-loops into account\n",
    "        if pair[0]!=pair[1] and not(pair[::-1] in network):\n",
    "            network.setdefault(pair,0)\n",
    "            network[pair] += 1 \n",
    "network_df = pd.DataFrame.from_dict(network, orient=\"index\")\n",
    "network_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 448,
     "status": "ok",
     "timestamp": 1641477104201,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "xN92q-Xp8nHN",
    "outputId": "17002944-6569-4c1b-e072-9da8601194cc"
   },
   "outputs": [],
   "source": [
    "#This cell is for Network2\n",
    "# NETWORK OF ONLY WORDS\n",
    "networkWords = {}\n",
    "networkWords_key = 0\n",
    "for index, row in tweets_filtered.iterrows():\n",
    "    word_list =[word for word in str.split(row[\"clean_text\"], \" \") if word in uw]\n",
    "    #itertool product creates Cartesian product of each element in the word list\n",
    "    for pair in itertools.product(word_list, word_list):\n",
    "        #exclude self-loops and count each pair only once because our graph is undirected and we do not take self-loops into account\n",
    "        if pair[0]!=pair[1] and not(pair[::-1] in networkWords):\n",
    "            networkWords.setdefault(pair,0)\n",
    "            networkWords[pair] += 1 \n",
    "networkWords_df = pd.DataFrame.from_dict(networkWords, orient=\"index\")\n",
    "networkWords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1641476701687,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "l2FhvB7i8oVL",
    "outputId": "0f033592-7930-4d81-b983-c2e5a0823253"
   },
   "outputs": [],
   "source": [
    "#This cell is for Network3\n",
    "# NETWORK OF ONLY HASHTAGS\n",
    "networkHashtags = {}\n",
    "networkHashtags_key = 0\n",
    "for index, row in tweets_filtered.iterrows():\n",
    "    #hashtags extracted from Tweet do not have the # sign in front of them but we will add it to differentiate hashtags from words\n",
    "    hashtag_list = ['#'+hashtag for hashtag in row[\"hashtags\"] if '#'+hashtag in unique_hashtags]\n",
    "    #itertool product creates Cartesian product of each element in the word list\n",
    "    for pair in itertools.product(hashtag_list, hashtag_list):\n",
    "        #exclude self-loops and count each pair only once because our graph is undirected and we do not take self-loops into account\n",
    "        if pair[0]!=pair[1] and not(pair[::-1] in networkHashtags):\n",
    "            networkHashtags.setdefault(pair,0)\n",
    "            networkHashtags[pair] += 1 \n",
    "networkHashtags_df = pd.DataFrame.from_dict(networkHashtags, orient=\"index\")\n",
    "networkHashtags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1641476711598,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "8uThrYGHSdEe",
    "outputId": "00a98b59-7de9-4ed6-e75e-721c45015976"
   },
   "outputs": [],
   "source": [
    "#This cell is for Network1\n",
    "network_df.reset_index(inplace=True)\n",
    "network_df.columns = [\"pair\",\"weight\"]\n",
    "network_df.sort_values(by=\"weight\",inplace=True, ascending=False)\n",
    "network_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1641477110323,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "PpfdMCeq9DKw",
    "outputId": "baeb7c2a-9ebf-4f51-b29a-63de01d774e0"
   },
   "outputs": [],
   "source": [
    "#This cell is for Network2\n",
    "# FOR WORD NETWORK\n",
    "networkWords_df.reset_index(inplace=True)\n",
    "networkWords_df.columns = [\"pair\",\"weight\"]\n",
    "networkWords_df.sort_values(by=\"weight\",inplace=True, ascending=False)\n",
    "networkWords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1641477146234,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "iZ_Eqf4d9Kyo",
    "outputId": "d1a56d7e-c80c-47b0-dccf-5b0b20ad7f9f"
   },
   "outputs": [],
   "source": [
    "#This cell is for Network3\n",
    "# FOR HASHTAG NETWORK\n",
    "networkHashtags_df.reset_index(inplace=True)\n",
    "networkHashtags_df.columns = [\"pair\",\"weight\"]\n",
    "networkHashtags_df.sort_values(by=\"weight\",inplace=True, ascending=False)\n",
    "networkHashtags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJvNvzGXy8Kg",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This cell is for Network1\n",
    "#to get weighted graph we need a list of 3-element tuplels (u,v,w) where u and v are nodes and w is a number representing weight\n",
    "up_weighted = []\n",
    "for edge in network:\n",
    "    #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
    "    #if(network[edge])>1:\n",
    "    up_weighted.append((edge[0],edge[1],network[edge]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(up_weighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YY9YphU9WbK"
   },
   "outputs": [],
   "source": [
    "#This cell is for Network2 & Network3\n",
    "# list of 3-element tuples for word and hastag networks\n",
    "up_weighted_words = []\n",
    "for edge in networkWords:\n",
    "    #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
    "    #if(network[edge])>1:\n",
    "    up_weighted_words.append((edge[0],edge[1],networkWords[edge]))\n",
    "\n",
    "G_words = nx.Graph()\n",
    "G_words.add_weighted_edges_from(up_weighted_words) # words graph\n",
    "\n",
    "#to get weighted graph we need a list of 3-element tuplels (u,v,w) where u and v are nodes and w is a number representing weight\n",
    "up_weighted_hashtags = []\n",
    "for edge in networkHashtags:\n",
    "    #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
    "    #if(network[edge])>1:\n",
    "    up_weighted_hashtags.append((edge[0],edge[1],networkHashtags[edge]))\n",
    "\n",
    "G_hashtags = nx.Graph()\n",
    "G_hashtags.add_weighted_edges_from(up_weighted_hashtags) # hashtags graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 41665,
     "status": "ok",
     "timestamp": 1641477366035,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "PhfOH64iajWZ",
    "outputId": "0379e2e5-9dd4-4c51-db5e-c50cae42d036"
   },
   "outputs": [],
   "source": [
    "# No need to run this cell, we'll use Gephi for visualization anyways\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "nx.draw(G, with_labels=True, node_size=1.5, alpha=0.3, arrows=True)\n",
    "plt.show()\n",
    "nx.draw(G, with_labels=True)\n",
    "plt.show()\n",
    "\n",
    "### ploting words network\n",
    "nx.draw(G_words, with_labels=True, node_size=1.5, alpha=0.3, arrows=True)\n",
    "plt.show()\n",
    "nx.draw(G_words, with_labels=True)\n",
    "plt.show()\n",
    "\n",
    "### ploting hashtags network\n",
    "nx.draw(G_hashtags, with_labels=True, node_size=1.5, alpha=0.3, arrows=True)\n",
    "plt.show()\n",
    "nx.draw(G_hashtags, with_labels=True)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 364,
     "status": "ok",
     "timestamp": 1641477443087,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "eSneLIqZNvt1",
    "outputId": "bc3acef6-8a04-4d3e-db02-8b960fb3f16b"
   },
   "outputs": [],
   "source": [
    "# This cell is for Network1\n",
    "# WORDS&HASHTAGS NETWORK NODES&EDGES\n",
    "print(len(G.nodes())) # nodes=each word in the texts\n",
    "print(len(G.edges())) # edges=pairs for each word\n",
    "#G.edges\n",
    "#G.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1641477467236,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "yfj-H9U49n02",
    "outputId": "1d449394-5fa5-46fc-ae58-3f65a64c95d1"
   },
   "outputs": [],
   "source": [
    "# This cell is for Network2 & Network3\n",
    "\n",
    "#WORDS NETWORK NODES&EDGES\n",
    "print(len(G_words.nodes()))\n",
    "print(len(G_words.edges()))\n",
    "#HASHTAGS NETWORK NODES&EDGES\n",
    "print(len(G_hashtags.nodes()))\n",
    "print(len(G_hashtags.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1641477541330,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "b3avJtbl-A5q",
    "outputId": "72e92610-f346-45ed-989f-fc6e13e84d05"
   },
   "outputs": [],
   "source": [
    "#In this cell, we rank the nodes using Pagerank function and print the top20 nodes with the highest pageranks\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "pr = nx.pagerank(G, alpha=0.9) # we ranked the nodes of Network1 with pagerank\n",
    "\n",
    "# Let's print the top-20 pairs\n",
    "c = Counter(pr)\n",
    "top_20_network = c.most_common(20) \n",
    "print(top_20)\n",
    "print(\"********************\")\n",
    "\n",
    "pr_word = nx.pagerank(G_words, alpha=0.9) # we ranked the nodes of Network2 with pagerank\n",
    "\n",
    "# Let's print the top-20 pairs\n",
    "c = Counter(pr_word)\n",
    "top_20_word = c.most_common(20) \n",
    "print(top_20_word)\n",
    "print(\"********************\")\n",
    "\n",
    "\n",
    "pr_hashtag = nx.pagerank(G_hashtags, alpha=0.9) # we ranked the nodes of Network3 with pagerank\n",
    "\n",
    "# Let's print the top-20 pairs\n",
    "c = Counter(pr_hashtag)\n",
    "top_20_hashtag = c.most_common(20) \n",
    "print(top_20_hashtag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj3CwR5Cy8Kk"
   },
   "source": [
    "#### SAVE EDGELIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFtpm869ONHg"
   },
   "outputs": [],
   "source": [
    "# Now we're going to save edgelists and nodelists to use for Gephi later\n",
    "# Change the names according to your case\n",
    "filename1 = \"./network_edgelist_trans.csv\"\n",
    "filename2 = \"./word_edgelist_trans.csv\"\n",
    "filename3 = \"./hashtag_edgelist_trans.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTmGSBc3y8Kn"
   },
   "outputs": [],
   "source": [
    "nx.write_weighted_edgelist(G, filename1, delimiter=\",\") # Graph name must be changed!!\n",
    "nx.write_weighted_edgelist(G_words, filename2, delimiter=\",\") # Graph name must be changed!!\n",
    "nx.write_weighted_edgelist(G_hashtags, filename3, delimiter=\",\") # Graph name must be changed!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlboURoYy8Kp"
   },
   "outputs": [],
   "source": [
    "# If this cell doesn't work in your device, no worries you can skip it\n",
    "#add header with appropriate column names (works on collab and Linux/Mac(?))\n",
    "!sed -i.bak 1i\"Source,Target,Weight\" ./network_edgelist_trans.csv # Graph name must be changed!!\n",
    "!sed -i.bak 1i\"Source,Target,Weight\" ./word_edgelist_trans.csv # Graph name must be changed!!\n",
    "!sed -i.bak 1i\"Source,Target,Weight\" ./hashtag_edgelist_trans.csv # Graph name must be changed!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5oT2lSry8Kq"
   },
   "source": [
    "### Create Node List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1641478010814,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "lpef5RKvUu_w",
    "outputId": "ad0da51d-ab08-408d-e3a2-32323a5de591"
   },
   "outputs": [],
   "source": [
    "# This creates a csv file of nodes for Network2\n",
    "word_nodes = pd.DataFrame.from_dict(unique_words,orient=\"index\")\n",
    "word_nodes.reset_index(inplace=True)\n",
    "word_nodes[\"Label\"] = word_nodes[\"index\"]\n",
    "word_nodes.rename(columns={\"index\":\"Id\",0:\"delete\"},inplace=True)\n",
    "word_nodes = word_nodes.drop(columns=['delete'])\n",
    "\n",
    "word_nodes\n",
    "word_nodes.to_csv(\"word_nodelist_trans.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1641478017841,
     "user": {
      "displayName": "Mustafa Algun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8neAD8pWo4clblXdS6rgHQO_PTZV38FEIy3Lh=s64",
      "userId": "02151313343513449482"
     },
     "user_tz": -60
    },
    "id": "ZMdIcS4my8Ks",
    "outputId": "ab7f1452-a68d-493f-bbe9-c2c8aa511b56",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This creates a csv file of nodes for Network3\n",
    "hashtag_nodes = uh_df.copy()\n",
    "hashtag_nodes[\"Label\"] = hashtag_nodes[\"Hashtag\"]\n",
    "hashtag_nodes.rename(columns={\"Hashtag\":\"Id\"},inplace=True)\n",
    "hashtag_nodes = hashtag_nodes.drop(columns=['Count'])\n",
    "hashtag_nodes\n",
    "hashtag_nodes.to_csv(\"hashtag_nodelist_trans.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHZHIG18ye0F"
   },
   "source": [
    "#### SAVE NODELIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpE0P0cIU2OD"
   },
   "outputs": [],
   "source": [
    "# This joins the two nodelists above and creates a csv file of nodes for Network1\n",
    "nodelist = hashtag_nodes.append(word_nodes, ignore_index=True)\n",
    "nodelist\n",
    "nodelist.to_csv(\"network_nodelist_trans.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryR3LiMVB-70"
   },
   "source": [
    "Tasks: \n",
    "\n",
    "*   Extract username of user who posted the tweet into a column \"screen_name\". Follow the procedure we used to get the hashtags.\n",
    "*   Create a network of users using the mention relation. Is this a directed or undirected graph?\n",
    "*   We created a network where nodes are mixed (both words and hashtags). Create network of words only and one of hashtags only.\n",
    "* Pick one of these network and rank the nodes using PageRank centrality. Extract information about top-20 rated nodes.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PROJECT.ipynb adlı not defterinin kopyası",
   "provenance": [
    {
     "file_id": "11joAJSU6DiU66i_fPfZpwD1P99GVFmiN",
     "timestamp": 1640290795210
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
